model:
  model_name: bert-base-multilingual-cased
  tokenizer_name:
  adapters:
    - name_prefix: language
      reduce_factor: 4
      freeze: true
      pretrained_weights: LA/en_prev.pkl
    - name_prefix: task
      reduce_factor: 4
      freeze: false
      pretrained_weights:

train:
  seed: 42
  per_device_train_batch_size: 16
  per_device_validation_batch_size: 32
  train_logging_steps: 100
  num_train_epochs: 5
  warmup_steps: 100
  learning_rate: 1e-5
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  weight_decay: 0.0

dataset:
  dataset_name: Tevatron/msmarco-passage
  dataset_language: default
  dataset_split: train
  preprocess_num_workers: 16
  num_workers: 16
  train_n_passages: 8
  q_max_len: 16
  p_max_len: 128

  